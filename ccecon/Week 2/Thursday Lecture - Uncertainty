Below is an export of the slides for the lecture, although it loses
much of the content since that is not text:
===
Week 2: Uncertainty
Resources for learning R
Conveniently linear relationship
Uncertainty topics
Uncertainty in the ECS
Sources of uncertainty
Contributions
Expected utility

Other big messages from Climate Science
Lots of uncertainty
Policy, parameters, model, internal variability
Even more uncertainty in local changes


What will doubling CO2 do?
What will doubling CO2 do?
The equilibrium climate sensitivity (ECS) is the eventual change in GMST resulting from a doubling of CO2.
People also use the transient climate response (TCR), the change in GMST at  the time when a 1%-per-year increase  in CO2 results in a doubling of CO2.
Chaos in the climate system
Long-tailed climate sensitivity
Multiple lines of evidence
Process evidence:
Global climate models
Feedback loops
Historical record



Paleoclimate record

Distributions of future warming
Uncertainty in climate sensivity
Uncertainty in climate sensivity
Uncertainty in climate sensitivity
Pause for Practicum
Revisiting long-tails: recent measurements
Median ΔT by 2200:
SSP1-2.6: 1.8 C
SSP3-7.0: 6.1 C
Probability of ΔT > 3C under low-emissions scenario: 2.5%
Probability of ΔT > 9C under business-as-usual scenario: 2.4%.
Aside: Two Views on Random Variables
Random Variables are functions:  
Random Variables are unknowns that can take many possible values, according to a probability density function.
Best way to analyze is with many “draws”.
Monte Carlo analysis:
Turn all of the random variables into known values.
Calculate whatever you want to calculate.
Do it a million times.

So what?
Climate feedbacks determine final temperature change.
These include things like water vapor feedback, artic sea-ice melt rates, methane release, biosphere tipping points.
We don’t know these well, so the strength of the feedback is not well known.
Uncertainty in feedback processes results in long-tailed distributions: the chance of very high warming is not ignorable.

Uncertainty is a fact of climate
Each emissions scenario results in a range of possible temperature changes.
Within-process uncertainty
Within-process uncertainty
Within-process uncertainty
Within-process uncertainty
Within-process uncertainty
Within-process uncertainty
Uncertainty in the climate system
===

In the middle of lecture, we had a mini R practicum. Here is the jupyter notebook from that:
===
```R
xx = rnorm(1000, 0, 1)
```


```R
hist(xx)
```


    
![png](output_1_0.png)
    



```R
xx = rnorm(1000000, 3.5, 1)
```


```R
hist(xx)
```


    
![png](output_3_0.png)
    



```R
sum(xx > 6)
```


6336



```R
sum(xx > 6) / 1000000
```


0.006336


When there is an increase in downward radiation, surface temperatures change. This is expressed by
$$\Delta T = \lambda\, \Delta F$$
where $\Delta T$ is the change in surface temperature, $\Delta F$ is the change in downward radiation, called the radiative forcing, and $\lambda$ is some proportionality constant.

However, since a change in surface temperature will result in further radiative forcing. Let's model this:

Step 1. Initial change in forcing increases temperature:
$$\Delta T_0 = \lambda\, \Delta F_0$$

Step 2. Change in temperature causes additional forcing:
$$\Delta F_1 = C\, \Delta T_0$$

Step 3. Repeat steps 1 and 2 as follows:
$$\Delta T_i = \lambda\, \Delta F_i$$
$$\Delta F_{i+1} = C\, \Delta T_i$$
until $\Delta T_i$ goes to 0.

Now let's parameterize this:

"In the absence of feedback processes, climate models show λ ≡ λ0 = 0.30 to 0.31 [K/(W/m2)] (where λ0 is the reference climate sensitivity) (16), giving an equilibrium increase ΔT0 ≈ 1.2°C in response to sustained 2 × CO2." (Roe & Baker 2007)

"For the purposes of illustration, a normal distribution in hf(f) is shown with a mean of 0.65 and a SD of 0.13, typical to that obtained from feedback studies of GCMs (17, 18)." (Roe & Baker 2007)


```R
lambda = 0.30 # K/(W/m^2)
CC = 0.65 / lambda # this comes from f = lambda C
dF0 = 1.2 / lambda # W/m^2
```


```R
results = data.frame(tt=0, dF=dF0, dT=lambda * dF0)
```


```R
results
```


<table class="dataframe">
<caption>A data.frame: 1 × 3</caption>
<thead>
	<tr><th scope=col>tt</th><th scope=col>dF</th><th scope=col>dT</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>0</td><td>4</td><td>1.2</td></tr>
</tbody>
</table>




```R
results = data.frame(tt=0, dF=dF0, dT=lambda * dF0)
for (tt in 1:100) {
    dF = CC * results$dT[nrow(results)]
    results = rbind(results, data.frame(tt, dF, dT=lambda * dF))
}
```


```R
results
```


<table class="dataframe">
<caption>A data.frame: 101 × 3</caption>
<thead>
	<tr><th scope=col>tt</th><th scope=col>dF</th><th scope=col>dT</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td> 0</td><td>4.000000e+00</td><td>1.200000e+00</td></tr>
	<tr><td> 1</td><td>2.600000e+00</td><td>7.800000e-01</td></tr>
	<tr><td> 2</td><td>1.690000e+00</td><td>5.070000e-01</td></tr>
	<tr><td> 3</td><td>1.098500e+00</td><td>3.295500e-01</td></tr>
	<tr><td> 4</td><td>7.140250e-01</td><td>2.142075e-01</td></tr>
	<tr><td> 5</td><td>4.641163e-01</td><td>1.392349e-01</td></tr>
	<tr><td> 6</td><td>3.016756e-01</td><td>9.050267e-02</td></tr>
	<tr><td> 7</td><td>1.960891e-01</td><td>5.882673e-02</td></tr>
	<tr><td> 8</td><td>1.274579e-01</td><td>3.823738e-02</td></tr>
	<tr><td> 9</td><td>8.284765e-02</td><td>2.485430e-02</td></tr>
	<tr><td>10</td><td>5.385097e-02</td><td>1.615529e-02</td></tr>
	<tr><td>11</td><td>3.500313e-02</td><td>1.050094e-02</td></tr>
	<tr><td>12</td><td>2.275204e-02</td><td>6.825611e-03</td></tr>
	<tr><td>13</td><td>1.478882e-02</td><td>4.436647e-03</td></tr>
	<tr><td>14</td><td>9.612735e-03</td><td>2.883821e-03</td></tr>
	<tr><td>15</td><td>6.248278e-03</td><td>1.874483e-03</td></tr>
	<tr><td>16</td><td>4.061381e-03</td><td>1.218414e-03</td></tr>
	<tr><td>17</td><td>2.639897e-03</td><td>7.919692e-04</td></tr>
	<tr><td>18</td><td>1.715933e-03</td><td>5.147800e-04</td></tr>
	<tr><td>19</td><td>1.115357e-03</td><td>3.346070e-04</td></tr>
	<tr><td>20</td><td>7.249818e-04</td><td>2.174946e-04</td></tr>
	<tr><td>21</td><td>4.712382e-04</td><td>1.413715e-04</td></tr>
	<tr><td>22</td><td>3.063048e-04</td><td>9.189145e-05</td></tr>
	<tr><td>23</td><td>1.990981e-04</td><td>5.972944e-05</td></tr>
	<tr><td>24</td><td>1.294138e-04</td><td>3.882414e-05</td></tr>
	<tr><td>25</td><td>8.411896e-05</td><td>2.523569e-05</td></tr>
	<tr><td>26</td><td>5.467733e-05</td><td>1.640320e-05</td></tr>
	<tr><td>27</td><td>3.554026e-05</td><td>1.066208e-05</td></tr>
	<tr><td>28</td><td>2.310117e-05</td><td>6.930351e-06</td></tr>
	<tr><td>29</td><td>1.501576e-05</td><td>4.504728e-06</td></tr>
	<tr><td>⋮</td><td>⋮</td><td>⋮</td></tr>
	<tr><td> 71</td><td>2.084051e-13</td><td>6.252153e-14</td></tr>
	<tr><td> 72</td><td>1.354633e-13</td><td>4.063899e-14</td></tr>
	<tr><td> 73</td><td>8.805115e-14</td><td>2.641534e-14</td></tr>
	<tr><td> 74</td><td>5.723325e-14</td><td>1.716997e-14</td></tr>
	<tr><td> 75</td><td>3.720161e-14</td><td>1.116048e-14</td></tr>
	<tr><td> 76</td><td>2.418105e-14</td><td>7.254314e-15</td></tr>
	<tr><td> 77</td><td>1.571768e-14</td><td>4.715304e-15</td></tr>
	<tr><td> 78</td><td>1.021649e-14</td><td>3.064948e-15</td></tr>
	<tr><td> 79</td><td>6.640720e-15</td><td>1.992216e-15</td></tr>
	<tr><td> 80</td><td>4.316468e-15</td><td>1.294940e-15</td></tr>
	<tr><td> 81</td><td>2.805704e-15</td><td>8.417113e-16</td></tr>
	<tr><td> 82</td><td>1.823708e-15</td><td>5.471123e-16</td></tr>
	<tr><td> 83</td><td>1.185410e-15</td><td>3.556230e-16</td></tr>
	<tr><td> 84</td><td>7.705165e-16</td><td>2.311550e-16</td></tr>
	<tr><td> 85</td><td>5.008357e-16</td><td>1.502507e-16</td></tr>
	<tr><td> 86</td><td>3.255432e-16</td><td>9.766297e-17</td></tr>
	<tr><td> 87</td><td>2.116031e-16</td><td>6.348093e-17</td></tr>
	<tr><td> 88</td><td>1.375420e-16</td><td>4.126260e-17</td></tr>
	<tr><td> 89</td><td>8.940231e-17</td><td>2.682069e-17</td></tr>
	<tr><td> 90</td><td>5.811150e-17</td><td>1.743345e-17</td></tr>
	<tr><td> 91</td><td>3.777248e-17</td><td>1.133174e-17</td></tr>
	<tr><td> 92</td><td>2.455211e-17</td><td>7.365633e-18</td></tr>
	<tr><td> 93</td><td>1.595887e-17</td><td>4.787661e-18</td></tr>
	<tr><td> 94</td><td>1.037327e-17</td><td>3.111980e-18</td></tr>
	<tr><td> 95</td><td>6.742623e-18</td><td>2.022787e-18</td></tr>
	<tr><td> 96</td><td>4.382705e-18</td><td>1.314811e-18</td></tr>
	<tr><td> 97</td><td>2.848758e-18</td><td>8.546275e-19</td></tr>
	<tr><td> 98</td><td>1.851693e-18</td><td>5.555078e-19</td></tr>
	<tr><td> 99</td><td>1.203600e-18</td><td>3.610801e-19</td></tr>
	<tr><td>100</td><td>7.823402e-19</td><td>2.347021e-19</td></tr>
</tbody>
</table>




```R
sum(results$dT)
```


3.42857142857143



```R
plot(cumsum(results$dT))
```


    
![png](output_17_0.png)
    


Now, let's do some Monte Carlo runs:


```R
results = data.frame()
for (ii in 1:1000) {
    ff = rnorm(1, .65, .13)
    CC = ff / lambda # this comes from f = lambda C

    dT = lambda * dF0
    DeltaT = dT
    for (tt in 1:100) {
        dF = CC * dT
        dT = lambda * dF
        DeltaT = DeltaT + dT
    }
    
    results = rbind(results, data.frame(ff, DeltaT))
}
```


```R
hist(results$ff)
```


    
![png](output_20_0.png)
    



```R
plot(density(results$ff))
```


    
![png](output_21_0.png)
    



```R
hist(results$DeltaT[results$DeltaT < 700])
```


    
![png](output_22_0.png)
    



```R
hist(results$DeltaT[results$DeltaT < 10])
```


    
![png](output_23_0.png)
    

===

Finally, here is a transcript of the whole lecture:
===
Now  having  vs the  first. Okay? So  I'm  going to  start  by  doing  the  last  slide  of the  previous  Fletcher  and  then we're  off  on  to  talk  about  uncertainty. One  of  the  things  that  I wanted  to  this  semester with  this  is  not  a  in the  sense  that  we  do  a  bunch  of putting  one  benefit  of  being  able  to pop  open  or  anytime we  want  is  that  we  can  take  some  of the  figures  that  we've  been  going  through the  lecture  and  dig  into  them and  figure  out  what  it looks  like  to  analyze  data. I'm  not  positive  that the  weight  give  to  that today  is  going  to  particularly  enlighten  you, but  I'm  positive  that the  thing  that  I  want  to  dig  into is  fairly  opaque  without a  little  bit  of  extra  DDD. By  the  way,  there's  some  questions about  additional  resources  for  learning. Are  I  made  a  little  page on  campus  with  some  of the  resources  that  I've  been  pointed  to. There's  a,  there's  a  book  data  science that  looks  quite  good. I  was  pointing  to  some  Youtube  tutorials and  nowadays BT  TA  is  just fantastic  for  answering  question, so  there's  some  things  to. Okay,  like  I  said last  time  there's  this  crazy  new  insight, so  feels  pretty  new. I  guess  it  was first  pointed  out  a  couple  of  CC's  ago that  the  more  CO  two  we  have  in the  am  warming  happens  and  that those  two  have  basically a  linear  relationship  between. This  is  cumulative  CO  two  emissions  since 18  50  so  basically since  the  beginning  of  industrial  revolution. Black  here  is historical  warming  as  you  can  see. The  black  line  ends  around 2,500  tons  of  CO  two. We  are  right  at 1.2  or  something  degrees  warming. So  if  you  that  looks  really line  then  each  of  these  is a  global  planet  bottle that  projects  out  further, putting  two  an  atmosphere  further  warming. You  can  see  lines basically  all  way  on  top  of  each  other. It's  not  at  all  clear  why  there would  be  such  a  linear  relationship. There's  lots  of thinking  about  tipping  points, the  climate  system, this  suggest  that  there  should not  be  a  particular  linear  relationship  here. That  if  you  add  more  CO, two  past  a  particular  point, you  can  imagine  the Earth  system  triggering  something  in the  Earth  system  that causes  warming  to  go  off  the  rails. One  possibility  for  example  of doing  that  is  Herma  Frost. We  know  that  as  we  um, as  temperatures  across  permafrost  areas, as  above  critical  thresholds, the  layer  of  permafrost that  has  been  stable  and  has captured  a  lot  of  methane  for thousands  of  years  is  going to  start  releasing  methane. And  so  you  can  imagine  this  graph looking  so  that,  okay, at  3,500  so  suddenly the  warming  that  you  get  tilt  up  a  lot. It  turns  out  that  that  isn't  the  case. It  suggests  that  if  there  are, well,  we  know  that  there are  these  tipping  points, but  it's  just  that  the  tipping  points have  actually  been happening  underneath  all  the  changes in  warning  that  we've  experienced  to  date. And  the  tipping  points that  will  happen  in  the  future are  differ  from  the  points that  have  been  happening  at. This  may  not  turn  out  to be  the  truth  of the  way  that  the  cleaning  system  works, but  at  least  the  best  evidence  is now  that  the  C  system  is  fairly  linear. And  that's  for  people like  me  who  don't  want  to spend  all  their  time  doing  science, but  want  to  turn  some  of  this into  economic  decision  making. So  we'll  come  back  to  this  later,  Okay. One  of  the  things  you  see here  is  already  there. There's  a  lot  of  uncertainty  around  that. So  at  any  given  level  two  emissions, there's  uncertainty  about  how  much  warming we're  going  to  get  actually. There's  also  uncertainty  about how  much  warming  we  currently  have  today. We  don't  really  know  what  the  global  mean, surface  temperature  is  today because  there  are  a  lot  of  regions  where  we just  don't  have  good  data, we  don't  know  what  it  is  in  a  lot of  the  very  core  regions  of  the  world. I  mentioned,  um,  the  Arctic  and  Antarctic, we  don't  know  what  it  is across  large  parts  of the  ocean  where  we  don't  have that  many  places  we  measured. But  that's  just  the  beginning  of, there  are  all  sorts of  processes  within  the  climate  system that  we  don't  properly  understand, the  potential  for  those  processes to  send  us  off into  different  kinds of  temperature,  warming  scenarios. It  is  pretty  high,  so  we'll  talk  about  that. I'm  going  to  start  with  talking about  uncertainty  in a  particular  variable  called the  equilibrium  kind  of  sensitivity, which  is  pretty  important  for  all  of  this. Talking  a  little  about  how  to  think about  different  sources  of  uncertainty. How  to  think  about  how uncertainty  contributes to  components  of  uncertainty, contributes  to  total  uncertainty. And  if  we  can  get  into  it  to  expect utility  in  the  way  that  you can  handle  that  in  economics,  okay. So  one  of  the  big,  I have  to  slide  like  this  before, big  messages  from  client  side. One  of  the  big  messages  from science  is  that  there's pervasive  uncertainty. That  uncertainty  has  a  lot of  different  common  forms. Policy  uncertainty,  we  don't  know exactly  what  oriented  parameter  uncertainty, the  things  like  these. Yes.  Which  we're  talking  about. Model  uncertainty  models  degree, internal  variability, that  there's even  more  uncertainty  and  local  changes. A  third  point  that  I  should  have here  is  most  of these  sources  of uncertainty  decomponents,  which  means  that We  don't  know  the, we  don't  know  the  quantitative  nature of  the  insert  in  cases  experts  disagree about  the  right  way  to model  that  uncertainty  and  then since  they  have  different  parameters that  they  used  to  describe  the  uncertainty, you  can  get just  very  different  representations. We  don't  really  know  what  the  right  thing  is. Okay,  so  we've  talked  about  this  before, but  it  leads  into  a  discussion  of the  Equilibrium  Claims  Institute. What  will  doubling  of  CO  two  Nans. This  is  model  down here  at  290  parts  per  million. That  was  pre  industrial  level of  CO  two  in  the  atmosphere. Imagine  the  slides  show  where  it  goes, but  you  can  imagine  sort  of bottoming  out  in  predustrial  times. By  the  way,  predustrialdtri  revolution happens  somewhere  in  the  17, later  half  of  the  1700s. Often  when  people  are being  careful about  comparing  things  to  pre  industrial, they  compare  things  to  ten around  17  50  carefully  prestrial. We  don't  have  any  information about  what  things  were like  back  in  17  50. So  people  who  are  a  little bit  more  concerned  about the  data  problems  will often  define  prestrial  be  more  like  18  15. We  still  barely  have an  idea  of  what  was  going  on  in  18  50. Pbm  is  our  best  sense of  the  way  things  looked. In  the  1,700  doubling that  couple  of  29580  under  a  business  tt, we're  going  to  reach  a  doubling  of  CO, two  around  27 perameters in  a  lot  of  models. It's  actually  a  parameter in  the  high  quality  models, but  it's  emergent  property  of  those  models. A  key  emergent  property  for  some  models  and pre  others  is  called  the  El  clan, which  is  the  eventual  change  in GMSTfce  resulting  from  a  doubling of  CO  two  here. This  is  actually  a  graph  that also  transit  from  ritual  defining  moments. Double  CO  two  over  some  period  of  time. It  takes  a  while  for  the  temperature  to come  to  a  new  equilibrium  level, and  then  it  stabilizes  there. That  change  here  is measured  to  be  about  2.5  degrees. That  would  be  our  estimate  of the  equilibrium  in  this  graph. The  way  that,  that  doubling  was implemented  was  by  increasing the  level  of  CO  two  by  1%  a year  up  until  you  got  to  the  doubling  of  two. Now  it  doesn't  matter the  equilibrium  Csicide  or  how  you decide  to  accompany  two  mostre  to  get  there. Right?  Because  all  that  matters  is  that  at some  point  you  have  doubling  two  and  then just  wait  a  little  while  and then  you  come  to  equilibrium. And  you  measure  what  the  S  difference  is. The  benefit  of  having a  structured  way  to  increase  the  CO  two  in your  atmosphere  is  that  you can  figure  out  how  quickly  does the  Earths  respond  to  changes  in  Cs. You  could  instantaneously  double 02  and  then  see  how  long  it  takes  to get  90%  of  the  weight  to  the  equilibrium. That  would  be  one  way  to  measure how  quickly  the  Earth  responds. But  that's  not  a  realistic  scenario. We're  not  going  to  instantaneously double  the  amount  of  two. This  idea  of  1%  increase  per year  is  useful  because it's  something  more  like what  we're  actually  doing. And  then  what  we  do  is  you  say,  okay, when  I  get  to  that  doubling  of  CO two  basically  increase  by  percent  year  until, until  some  it's  about  70  years  in  the  future. Then  want  to  know what  the  temperature  is  at  that  point. It's  not  going  to be  the  equilibrium  pensusenough we're  at  the  equilibrium  CO, two  level,  but  it's going  to  be  on  the  way  there. So  you  can  measure  TCR as  something  like  1.5 degrees  in  the  equilibrium  census. The  last  thing  that  it  points  out  here is  at  any  given  point  in  time, there's  a  warming  command. We  put  a  certain  two  in the  atmosphere.  We're  not  at  equilibrium. That  means  that  the  Earth  is  in  the  process of  approaching  its  new  equilibrium. Well,  how  much  more  warming? Is  the  Earth  wind. You,  even  if  we  don't  put any  more  sat  in  the  atmosphere, that's  our  warming  Committed. Object.  Sorry,  I  don't  want  to  interrupt. A  really  good  thing  would be  the  thing  that  you  had last  semester  with  the  freezer and  you  turn  up  the  temperature. There's  a  bit  of  climate  inter  yeah,  exactly. So  this  is  this  is  a  kind of  inertia  in  the  system, so  usually  the  amount of  additional  warming  that we  committed  to  at  a  given  time, it  isn't  actually  so  much. So  like  the  tempature  is going  to  double  from  where  it  is  today. The  Earth  system  responds fairly  rapidly  to  increases  in  tempt. But  fairly  rapidly  means like  ten  to  40  years. It  takes  about  that  long  for  chic to  something  close  to includer  you  can  see  here, okay,  from  the  point where  were  we  got  to  here. If  this  is  saying  that it  takes  more  like  200  years, that's  actually, I  think  a  bit  of  old  evidence that  the  new  evidence  is  that, is  that  the  Earth  system responds  fairly  rapidly to,  to  two  emissions. Probably  if  we  were  to  stop  them, now  we  come  to a  nucum'reylose  to  the  nuclei within  the  next  year. Now  let  me  tell you  about  a  fun  little experiment  that  was  done. It  was  a  distributed  computing  experiment. These  people  at  a  group called  Climate  Prediction dont  a  bunch  of computers  to  all  work  together  to  run a  fairly  sophisticated  locale. These  are  big  models. Do  you  need  a  single  run  of  these  models? On  laptop? It  would  take,  it  would take  a  year  or  something to  get  a  single  run. The  only  way  they're  going  to  get some  sense  of  the  uncertainty  that's possible  from  these  models  is  either  by  using a  really  high  performance  computing  cluster or  having  lots  of  people run  parts  of  this  model  on their  laptops  and  then shared  that's  what  they  did. They  split  up  the  simulation that  they  did  into  three  phases. The  first  phase  is  where  they just  have  everything  at  baseline. Conditions  aren't  actually  simulating any  additional  CO  two  spy. Actually  worse  in  the  calibration  base, they're  not  allowing any  particular  reaction  of the  system  to the  simulation  in  the  control  base. They're,  they're  going  to  let  the  ass  respond the  stans  just going  to  lead  over  the  constant. You  can  already  see  that  there  are  a  bunch  of individual  simulations of  this  Earth  system  model. We  are  things  wacky. Most  of  these  were  the  Earth going  to  like  a  Snowball  Earth  situation. Big  decreases  in  global  non  surfaces. They  eliminated  all  of those  scenarios  that  we're  doing, something  that  was  not in  line  with  the  calibration  base. Then  for  those  simulations  that  remained, they  let  them  run  for another  another  20  years and  see  what  happens. One  thing  to  notice,  by  the  way, be  explicit  about  this,  when you  see  an  individual  line  here that's  one  individual  run  of  the  model. Probably  distributed over  100  different  computers. A  single  computer  would  be  able  to  do one  model  already  where  you have  that  means  just aren't  that  many  ones  that are  where  that  space  and  then  most  of the  runs  in  these  areas  where  you  see  the. Now  you  can  see that  even  after  this  control  base, after  they  eliminated  all  of  these  models, all  these  particular  simulation modes  where  things  are  going  wacky, there's  still  bizarrely  wide  range of  outcomes  that  you  can  see. There's,  there's  a  big  chunk  of the  simulations  that  go to  Snowball  Earth  conditions. There's  a  big  chunk  of  simulations that  go  up  to  plus  six  degrees  or  more. And  that's  just  over  the  course  of Do  you  guys  know  what  it  is  that's changing  across  these  fringe  models? Ideas  of  some  of  the  things that  they  vary  to  produce all  these  different  simulations  parameters, parameters. But  there  are  a  few  different  kinds  of model  parameters  of  the  big  things that  we  don't  know  very  well are  things  like  rate  of  ice  melt, formation  of  piles, the  response  of  the  biosphere  to  increase  02. Those  would  be  some  of  the  things that  would  be  varying  across  these. The  other  thing  though, that's  also  vary  across these  are  what  are  terms, initial  conditions,  your  preference, initial  conditions,  and  certainty. And  it  turns  out,  if  you change  the  initial  distribution  of, say,  temperatures  and  winds  and  pressures, which  are  the  main  things that  are  being  represented  in  the  bottles, then  you  can  get  very  different  things. What's  the  name  of  that? That  if  you  change  the  initial  conditions  get totally  different  results  all. Yes.  Yeah.  So  the  Earth  system, one  of  the  features  is  that  it has  the  butterfly  flight, you  have  the  potential that  butterfly  fuck  swings in  one  area  will  completely change  the  whole  future  of the  Earth  system  relative to  the  exact  same  world  where that  butterfly  didn't  fuck swings  in  that  particular  way. Time,  that's  a  feature  of  chaos  in  general. The  Earth  system  is  a,  is  a  chic  system. It  turns  out  that  if  there are  fairly  strong  negative  feedback  groups, that  are  the  feedback  loops that  constrain  the  behavior  of  a  system. That  those  butter  facts will  be  the  small  right. They  won't  make  things  go  wacky. Their  system  has  a  set  of  N  feedback  groups, but  it  also  has  positive  feedback  groups. Things  like  the  way that  Arctic  ice  causes there  to  be  additional  warming, cause  there  to  be  additional  warming  ice. The  situation  is, as  we  melt  ice  across  the  Arctic, we  reduce  the  reflect  tons of  the  Earth,  reduces  albedo. When  we  do  that,  the  Earth  warms  up  quickly, which  melts  the  ice which  makes  warm  up  more  quickly. And  you  can  have  this  runaway  effect. The  fact  that  most  of these  simulations  are  fairly  narrow  range. Those  are  the  simulations  that  where the  negative  feedback  loops dominate  for  the  most  part,  the  butterfly, the  butterflies  don't cause  things  to  go  crazy, this  white  foul  of  fewer  simulations, but  certainly  not  normal ones  where  things  go  crazy. One  important  point  consequence of  this  discussion  is that  although  we  may are  making  enormous  changes in  the  climate  system, that  the  dominant  driver of  things  that  we  see, even  if  we  weren't making  changes  in  the  system, we  might  still  have  large  if  we  were. So  how  large  changes  in  the  in  system, the  clin  system  has  the  ability to  have  these  big  changes. So  we  did  this  fun  experiment is  that  there  is  a  lot  of  uncertainty. And  that  the  distribution of  what  can  happen  when  you  have  the slit  two  is  pretty  wide. So  what  is  the  distributional,  right? This  is  the  actual  values  here. Basically  what  they  did  is they  double  the  CO  two, So  if  you  just  go  out to  this  0.20  years  in  the  future. 20  in  the  future,  by  the  way, is  not  yet  equilibrium  by  most  standards. But  this  was  done  15  years  ago. It's  really  tough  to  even do  20  years  in  the  future. They  took  the  sets of  simulations  and  they  turned  them. This  probabnsity  function, this  black  line  with  the. So  those  two  should  exactly  correspond. I  try  to  say  exactly  to  compare  them, but  that's  what  it, that's  what  it's  representing. This  on  the  x  axis  corresponds  to  that  on the  Y  axis  minus  the  pitial  condition  tenure. And  then  there  are  a  couple other  estimates  on  here  too. So,  these  are  all  estimates of  equilibrium  clearance. The  immediate  thing  to  notice  is that  these  are  not  Gcm  distributions. These  are  not,  not  only are  these  skewed,  there's, there's  a  long  tail  here  of potential  temptureslyre the  skewed  that  things  there's, there's  more  weight  to  the  right, there's  left,  but  that  there's  a  long  tail. We'll  look  a  little bit  at  the  consequences  of long  tails  in  the  R  part  of  today. But  what  I  want  you  to understand  by  long  tail  at  this  point  is  just that  there  is sort  of  non  ignoral  mass  of  mass. A  temperature  changes  that are  more  times  the. Isn't  it  a  little  unfair  though  because you  cut  off  like  a  chunk  from  the  other  end. I  don't  know  how  much  it  would  tribe  but like  yeah. Well,  it  turns  out  that  this, no  matter  how  you  calculated on  this  top  sign, is  much  more  one  tailed  than  sign. Okay. The  most  recent  evidence  or  most recent  study  that's  been  done, libs  came  out  as  part  of  the  new  ICC  process, was  one  of  the  big  collective  projects that  fit  into  the  ACC  that  basically looked  at  a  bunch  of different  ways  you  could  go about  calibrating  the  ECS from  different  sources  of  data. One  thing  we  could  do  is  you  can  take different  global  climate  models and  run  them  and  then  measure  what  the  ECS, and  that's,  that's  what  this  computation, that  approach  was  doing. And  that  gives  you  this  blue  curve  here. Long  tailed,  really  white. You  can  also  do  the  same  thing  with individual  feedback  groups  and also  the  blue  curve  in loops  that  we  want  to  try  to  estimate how  strong  those  felps  are  and then  turn  to  some  evidence. Historical  record  is  using the  fact  that  we  already  pump the  love  switch  atmosphere. Estimating  how  much temperature  change  we've  experienced, given  how  much  say  comes  in and  in  CCS  that  way. And  so  you  can  see  there's sort  of  pep  insurgency  around  how  to turn  historical  data  into these  different  espects  of  CS. The  different  red  lines  here  are actually  just  different  results from  one  of  these  papers. But  depending  on  which  paper  you  ask, we  could  have  an  ECS that's  more  like  three  degrees, or  ECS's,  it's  more  like  1.5  degrees. The  third  approach  is to  look  at  the  Paleo  Clinic  record. If  you  look  back  at  warm  phases and  ice  ages  in  the  distant  past, this  a  little  bit,  we  know  that  the  Earth  has gone  through  different  bases in  past  and  we  know  that  CO wo  has  corresponded  closely with  those  warm  phases. When  there's  been  a  warm  phase, we've  also  gotten  um,  a  low  02. The  atmosphere  corresponds  answer to  this  because  I  think  I  may  have  mentioned it  in  the  last  cust. Do  you  guys  know  which  of  those  two  signals, the  temperature  signal  that  goes  up  and down  and  the  CO two  signal  that  goes  up  and  down, have  historically  which  one is  historically  come  first? Has  historically  the  Earth spread  out  a  bunch  of  two, and  then  the  temperature  responded, or  has  temperature  increased  on  its  own, and  CO  two  has  responded. To  tempt. The  CO  two  has  responded  to  the  temperature. The  temperature  has  been  changing because  of  changes  in the  procession  of  the  howls clip  the  Earth  as a  result  of  temperature  change. Different  reservoirs  of  CO two  in  system  have pumped  CO  two  in  the  atmosphere. And  then  that's  caused  there  to  be  a  puzzle, feedback  which  caused  the  planet  to  warm. So  that's  tell  you  one  thing, which  is  that  there  are some  of  these  positive  Paclops  that  we  should be  concerned  about  given the  global  experiment  that we're  running  right  now. The  other  thing  to  note is  when  we  use  the  pal, climatological  records,  you  measure  some  CS. It's  a  little  tough  because  we  don't see  the  same  experiment  there. We're  seeing  two  rise  in  temperature  rise, but  it's  because  the  temperature caused  to  rise,  not  the  other. Not,  not  initially, but  actually  once  you  get  the  two  x here,  I  keep  thinking  them. So  that  gives  you,  that gives  you  another  equip. They  labels  S,  but  it's,  it's, it's  what  ECS  and  ECS  is  the  standard  for. I  don't  know  why  they  call  it  S. Okay.  So  you  have these  three  different  ways of  estimating  them. All  of  them  have  quite  long  right  tails. And  then  part  of this  paper  was  basically  say, because  these  are  covered independent  forms  of  evidence, we  can  combine  them  and  get a  better  estimate  of  the ECS  than  ever  before. And  that's  how  we  got  the  flat  curve, which  they're  called  baseline. They  get  is  that, is  that  ECS  is  a little  bit  better  constrained  than  before and  somewhere  between  2.5  4.5  degrees  for Sionxixis  is  the  mass  of  the P.  Tough  to  understand if  you're  not  used  to  see  PDF  graphs. But  basically  what  it  tells  you  is the  most  likely  value  of  PCS is  where  the  peak  is  three  degrees. But  you  can  measure  what the  likelihood  is  of  any  other  level  of  CS  by basically  asking  what  that's what  value  in  that  curve  houses whatever  is  on  the  axis. So  the  chance  of  having  a  five  degrees  is something  like  a  ten  chance of  having  a  three  degree  ECS. The  chance  of  having  a  seven degree  CS,  very  small. Let's  hundred  chance  of having  a  three  degree. Well,  this  is  why  I  want  to, I  want  to  do  a  little  stuff  to  get a  little  more  intuition for  people  who  are  used  to  looking  at  PPS. Show  you  one  more  thing before  users  take  that. When  with  different  scenarios,  you  can  say, what  temperature  should  I  get out  in  the  future under  different  levels  of  emissions? And  these  are  the  stars that  we're  going  to roll  over  the  course  of  the  semester. First,  some  of  the, each  RCP  2.6  it's  now  called  SSP  one D  2.6  is  a  high  mitigation, so  where  there's a  really  strong  international  effort to  reduce  emissions  and  by the  end  of  the  century  we  end  up  with  say, warming  at  the  end  of  the  century relative  to  1995, 1995,  we  already  had about  20  degrees  warming  and  saying,  okay, you'll  have  about  another 20  degrees  of  warming, we'll  be  at  about  1.6  degrees  warming, nearly  that  1.5  degree  level, the  pascuten  on  if we  are  in  the  middle  of  road  stereo, that's  R  4.5  then  we end  up  being  a  little  three  degrees  warming. If  we  go  to  high  emission  stereos, the  well  let's  call the  meaning  of  that  four  degrees  out  point agrees  to  almost  five  degrees  of. Okay,  I  want  to  do  one  more  thing to  explain  my  R  here. I'm  going  to  go  wait  on  this, I'm  going  to  do  a  little  bit  of quick  intro  stuff  with. Are.  So  we're  talking  about  random  numbers. Random  numbers  are  parts or  variables  that  we  don't know  exactly  what  value  they  take. And  there  are  different  ways of  dealing  with  and  numbers, the  mathematical  ways  of dealing  with  mathematical  numbers. We're  mostly  not  going  to  be doing  random  numbers  that  way. We're  going  to  be  doing  something called  Monte  Carlo  analysis. Monte  Carlo  analysis, you  try  out  different  values  that  the  and number  and  you  can  look  at different  sisticstt  show  you  that. Start  by  saying  that  I  want  you  band number  x  which  is  going  to  be a  normal  distribution  to  take  1,000  draws. It's  normal  distribution  with a  mean  of  zero  and  sanitation. Say  okay,  did  it?  Okay. So  this  is  a  histogram  of  the  aim. You  can  see  that  on  average  it's  zero. You  can  see  that  a  stem  deviation one  means  that  takes about  three  step  deviations until  there's  not  much  probability  mass and  just  could  say  about it  and  you  can  ask  some  questions  about  it. So  for  example  I  can say  well  what's the  probability  that  so  let's, let's  say  that  this  is,  this  is  the  warming of  um,  turn  to  something. Let's,  let's  come  up  with the  temperature  at  the  end  of  the  century, which  is,  let's  say  on  average  3.5  degrees. But  there  is some  amount  of  uncertainty  cation, let's  keep  it  saying  a  little  bit. The  exact  peaks  of these  different  history  bars  are  a little  bit  different  just  because we're  only  drawing  1,000  draws. Just  to  see  that,  let's  do draws  Mr.  And  now looks  almost  perfectly  symmetric.  Okay? But  now  we  can, we  can  ask  some  questions  like what's  the  probability  that the  actual  temperature  at the  end  of  the  century  isn't  the  3.5 degrees  that  we,  that  we  put  in. But  it's  actually  more  like. So  trees  or  more. So  if  I  say  some  of  x  x  cred  six, what  that  is  going  to  tell  me  is how  many  of  these  simulations, how  many  draws  of  the  normal  distribution were  more  than  six  degrees. And  says,  okay,  cousin  of  the  million  work. I  know  that  there  were  1  million  of  them. So  I  can  say  that about  0.6%  of  the  simulations are  more  than  six  degrees. So do  you  have other  questions  about  about you  can  use  Tdcram  stuff? You  have  you  played  with the  scripts  before  that? Just  let's, let's,  let's  do  my,  my  more  help  kids. Okay? To  go  back  to  my  slides  for  moment, I'm  going  to  walk  you  through an  argument  that's  made  in the  famous  paper  called, The  Authors  are  Growing  Baker  2007. Their  question  was,  why  is  it that  the  ECS  has  this  long  right  tail? What's  what  they  point  out is  that  there's  a  natural, we  know  that  there  are  feedback  groups between  a  change  in  radio  forcing  and  change in  temperature  which  cause the  change  in  if  we Oh,  that's  going  to  cause  or  be more  house  change  radio  force. That's  going  to  result  in change  of  tenure  change. Tempature  is  going  to  cause  water vapor  in  the  atmosphere, water  in  the  atmosphere,  more radio  forcing  on  and  on  and  on. And  eventually  come  some  new  equis for  reasons  that  I'll  show you  in  just  a  minute. It  turns  out  that  if  you  have, you  can  measure  how  strong  that  pack is  between  temperature  and  radia  force. You  can  relate  the  initial  increase  in temperature  caused  by  just  CO  two  to the  final  increase  in  tempature  caused  by the  pole  feedback  between CO  two  and  water  vapor. The  strength  of  the  feedback goes  in  the  denominator  here. What  this  is  showing  is  if the  strength  of  the  feedback  is  zero, there's  no  feedback  between radio  forcing  and  water  vapor. Then  the  initial  temperature  from  CO two  is  going  to  be  pi  temperature  change. This  is  one,  that's  what  this  is  here. If  we're  here,  that  means  that, that  there's  no  further  change  in  tenure. If  we're  at  a  feedback  level  of  one, that  means  that  we  are  in a  perfect  feedback  loop where  you  increase  tempature. That  produces  enough additional  water  right  in the  atmosphere  so  that  the  radio forcing  increases  as  much  as the  initial  CO  two  cause  it  to  increase. Which  means  that  you  have  an  equal  amount of  additional  water  in  the  atmosphere. Which  means  that  basically your  tempature  goes  to. Okay  here,  feedback  factor  one. You,  for  any  given  change  temperature, it'll  give  an  infinite  change  in final  temptures. Here's  the  problem.  We  don't know  the  exact  level  of  the  feedback. We  don't  know  how  strong  this  feedback  is. Let's  assume  that  there's, there's  some  evidence  that you  can  sort  of  estimate  that  it's  somewhere around  0.65  It  seems  like  it's, it's  fairly  likely,  but  fairly  likely. There's  this  uncertainty  around  it. And  let's  assume  that  unset  is perfectly,  normally,  you  know, long  tails  here  could  be  a  little  bit  higher than  the  0.65  or  than  65. Everything  is  one  that  we  don't have  any  long  tails. We  say  more  about  the  Conlon  tails  in  a  bit. But  intuitively,  the  consequence  of long  tails  is  what  happens  if your  value  happens  to be  near  the  temperature  changes. Fgfinity,  any  probability  mass you  have  that's  anywhere  out  that  way is  going  to  result  in  a very  large  potential  change  in. Now  if  we  take  the  0.65  measure, the  final  change,  still  get some  level  low  under  four  degrees. If  you  take  a  value  that's  like 0.4  also on  this  distribution,  you  get  a  lower  value. If  you  take  a  value  that's  up here  like  0.9  we  get a  very  value  that produces  a  new  probability  density  function. That  probability  function is  no  longer  normally distributed,  it  has  a  long  tail. That  long  tail  basically  means  that  there  is non  trivial  probability  mass  wholly up  to  infinite  train. Okay,  I'm  going  to  this  equation, that  0/1  It's, it's  not  difficult  at  all to  do  the  math  to  get  that  equation. But  I  want  to  break  it  down into  a  little  bit  more  of  a  step by  step  process  for  pedagogical  reasons. So  let's, let's  walk  through  the  steps of  the  text  here. So  when  there's  an  increase  in  now radiation  surface  tempature  changed, this  is  expressed  by  this  delta g.  So  bigger  is  land, this  and  factor  is just  a  portion  constant  forgiven, an  increase  in  radiative forcing  it downward  radiation,  but  it's  the  same  thing. Forgiven.  Increase  in  radio  forcing, you  have  to  give  increasemrew. That's  the  initial  change  in temptreaid  is  change  temperature, the  radiate  force  land is  proportionality  constant. This  change  in  surf  temptre  results  in further  radiative  forcing  through  defect. Um,  water  models,  the  first, the  first  step  is  going  to be  the  change  in  radiate forcing  from  two causes  a  change  in  temperature. Second  step  is  change  in tempature  causes  a  change  in  a  further  change in  radiated  forcing  is  just  another  parameter that  relates  basically how  much  water  aver  goes  into the  atmosphere  for  change  temperature. And  then  we  can repeat  those  two  steps  back  and forth  until  the  change in  temperature  basically  goes  to  zero. Why  would  it  go  to  zero? Because  assuming  that  the  feedback  loop  is less  feedback  effects  one each  time  you  do this  is  going  to  be  a  smaller  numbers. Here  are  a  couple  of  lines  for parameterizing  this  from  the  Baker  model. In  the  absence  of  feedback  processes, we  get  this  land  divide, which  is  very  closely  to  plan  to be  0.3  We  can  get  that  from  models. That  means  that  we're  going  to  get a  temperature  change  of  0.1 0.2  for  a  double  02. If  it  weren't  for  feedback, the  ECS  would  be  something  like  1.2  degrees. It  turns  out  that  the  actual  ECS  is more  like  three  to  five  degrees, or  three  to  four  degrees, which  means  that  actually the  feedback  effects  matter  of t.  Then  we're  going to  use  this  normal  distribution, which  has  for  how  strong  that feedback  effect  is  with the  meaning  of  0.65  sion. Three  also  calibrated  from  was  that  land. We  just  have  a  number, I'm  just  going  to  put  it in  0.3  just  like  it  said. The  CC  CCC. But  the  ation  was just  the  primeter  is  related to  the  primer  by feedback  is  letter  C. For  some  reason  the  paper, they  don't,  they  use the  letter  the  perimeter. They  would  give  you  a  value,  but they  give  you  the  permphy, give  you  the  premetervue  for  plant  is just  divided  by  land  perimeter. Land.  Now  lasting  we  need  is  how much  of  an  increase in  radiative  forcing  do  you  get? When  you  double  to the  experiment  by  double  two, that's  going  to  produce  some  acts  of  change. Radiate  forcing,  I  know  that because  it's  the  1.2  degrees  they would  get  the  change in  temperature  divided  by land's  just  the  Pre  here. So  I  know  that  one  degrees  is  how much  you  get  in  absence  of  feedback  effects. Lands  is  0.3  and  so  touch  is  whatever.  Okay? And  I'm  going  to  do  a  little  look  in  R. I  do  this  particular  form  of  a  look  a  lot. It's  not  the  most  elegant  way  to write  this  particular  calculation  in  R, but  it  shows  exactly  what  is  going  on. So  let  me  break  down  step one  line  is  going to  start  out  this  result  state of  frame  with  a  single  row  in  it. I'm  going  to  say  the  iteration  step of  the  simulation  is  one. I'm  going  to  call  that  T.  It's  not actually  time  because  this  isn't really  calibrate  time ation  initial  iteration  set, I  have  a  change  in  forcing of  four  watts  per square  meter  and  change  tempture  two, that's  the  full  effect  of just  the  step  two  is  100  times  over. I'm  going  to  run  the  full.  I'm  going  to  take the  change  in  temperature  initially, that's  going  to  be  1.2  degrees. I'm  going  to  specifically  grab  it from  the  last  row  of  this  data  frame. I'm  going  to  keep  adding  shot last  row  to  keep  moving  down. I'm  going  to  take  the last,  last  change  temperature by  sea  and  I'm  going  to  get the  changing  rated  forcing. And  then  I'm  going  to  complete the  loop  by  saying,  okay, I'm  going  to  add  this  new  row to  the  end  of  the  data  frame where  the  changing  rated forcing  is  what  I  just  speculated. And  the  change  in  temperature is  the  new  effect. The  further  change,  the  new  effect  of that  change  died  force.  That  makes  sense. All  right,  so  read  it.  Look  at  the  result. And  you  can  see that  you  usually  have,  let's  go  over  here. You  usually  have  two  degrees, and  then  you  add  the  water  vapor, and  that  increases  things  by  another 0.8  degrees,  0.78  degrees. And  then  you  add the  effect  of  that  water  vapor  and you  get  another  0.5  degrees. So  the  total  effect,  and  you  can  see, by  the  way,  you  know it  goes  to  zero  at  some  point. That  additional  water  vapor  that  you  get from  the  hundredth  iterations  of  this. I'm  going  to  then  just add  up  all  those  results. Let  me,  let  me  just  do  that  simple  way, some  results  DT. That's  going  to  be  the  total  effect of  doubling  CO  two  and  I get  a  ECS  value  of 54  degrees  within  the  ball  park. The  loop  works. This  is  a  slightly  different  way  of  doing  it. Cumulative  sum  is  basically taking  each  number  and  adding  it. Add  the  numbers  together  one  in  time. So  the  first  values could  be  just  the  first  value. Second  values  could  be  the  first and  second  value  added  together. Third  values  could  be  the  first,  third  value. So  that  gives  me  a  plot  that  looks  like  this, which  is  basically  the  amount  of warming  or  the  amount of  warming  after  a  certain  number of  loops  of  the  duration. So  care  12  goes  up  rapidly, there's  no  additional  effect  of  A.  Okay. Yeah,  I'm  going  to  do, I  did  that  so  I  could do  this  more  complicated  thing. It's,  it's  a  bit,  not  much. The  inner  part  of  this  is  going  to be  just  the  same calculations  that  I  just  did. What  I  want  to  do  now  though  is  I  want to  try  different  values  out for  the  feedback  Basically  I'm  doing, I'm  trying  to  different  values  here  and I  want  to  see  what  the  change temperature  is  that  I  get  out. I'm  going  to  use  the  exact same  distribution  that  they  did. Meaning  0.65  sedation  free. Now  my  new  results  already  is  going  to  be the  empty  I'm  going  to  do  1,000  differs. Wow. Draws  from  this  uncertainty distribution  feedback  value rather  than  just  being  0.65. Is  going  to  be a  random  number  that  is  drawn  from a  normal  distribution  with a  mean  0.65  intbutionot. From  that  I  can  get  the  value, all  the  other  values. The  fact  effect  of  the  other  values, just  like  I  did  before  do  little  100  times. Going  to  add  it  onto  my  data  frame because  it's  going  to,  so  again, but  I'm  going  to  end  up  adding  up all  the  little  changes  in temperature  until  I  get  the  final  change, tempature  after  100  iterations. And  then  I'm  going  to  add  that  where  say, okay,  here  I'm  going  to  my  data  frame. The  value  I  put  in  from  the  feedback  loop  and the  final  change  in temperature  from  aeration. This  recast  could  add  more if  this doesn't  give  you  the  result  that  I  want. Okay,  let's  start  with the  histogram  of  the  add  values. Make  sure  that  that's  right. So  this  is  a  histogram, it  doesn't  look  nice. And  if  the  Gaussian, the  way  that,  that  there  is. By  the  way,  there's a  different  question  in  called  density. Which  is  a  smooth  curve rather  than  a  histogram  curve. Still  it's  a  little  bit  up. That's  just  because  I  earn  1,000 draws  and  can  do this  histogram  of  the  change, tempt  what  happened. Why  does  it  say  that all  of  the  temperature  changes are  basically  zero,  right? Right.  So  the  axis  here goes  all  the  way  out  to  1,500  degrees. That  means  that  doubling  of  sit  two  results in  the  global  means  or  prempature, increasing  by  1,500  degrees. Hopefully  that  doesn't  happen, but  there's  no, these  equations  produce  that, as  that  happens  when  you  get  feedback. Um,  levels  that  are,  that  are  up  near  one, all  of  the  other  results were  in  this  silver  here. But  that's  partly  because that  one  value.  So  state  what  happens. We  dropped  that  one  value. That  same  results  less than  1,500  That  was  crazy. Come  on,  it  happened  again. I  just  fixed,  I  dropped  that,  that  outlier. So  there's  another  outlier, and  this  one  is  700. Okay,  let's,  let's  drop  that  one. Okay,  let's,  let's  do  less  than  700  degrees. Yeah.  The  defitely  cut  off. Oh,  look, look,  not  everything  is  right  over  there. Some  of  the  observations  are  at only  the  temperature  increasing by  25  degrees  or  so. This  is  the  nature  of a  can  tell  just  having  fun. But  this  is  the  nature  of a  long  tail  distribution. A  long  tail  distribution  will always  produce  not  just  one  outlier, but  sort  of  a  whole  string  of outliers  that  are  arbitrarily  large. When  I  tried  this  before, I  tried  to  home  the first  graph  that  I  showed  you, rather  than  showing  a  scale that  went  up  to  1,500  showing  x  scale, that  went  up  to  50,000  degrees. Temperature  change.  There's  no, there's  no  limit  to  the  kinds  of high  values  you  get  out of  a  long  tail  distribution. Unfortunately  that's,  that's  the  other  Mars. If  we  limit  this  to the  temperatures  that  we  expect, that  we'll  get  something  more  like  this. Where  okay,  the  most  likely  temperature is  a  change  of  2-4  degrees, so  that's  what  we  expect. But  that  there's  this  probability that  there  will  have temperatures  out  to  ten  degrees. And  you  just  saw  that  there  were  actually these  values  where  you  could have  a  probability  outcome that's  out  more  like  15  degrees. So  I  hope  that the  thermters  of  the  Earth do  not  produce  ten  degree. That  would  be  that  once  we say  questions  on  this. So  we  say  spend  all  my  time  around  Clemons's. Okay.  I  think  we  say  that a  lot  of  different  professions, if  it  turns  into  1,500 I  think  everyone  that say  that,  all  right,  that  makes, that  makes  me  because  just  like  to  be a  little  bit  of having  our  own  opinions  or  our  own  thing, everyone  is  going  to  say  it  met. Okay,  awesome. That  was  my  position. Now  this  was  a  very dumb  global  climate  model  I  just  showed  you. My  global  climate  model  had only  the  feedback  group and  didn't  have  anything. Any  of  the  Ndebackro  interesting  of the  high  powered  local  climate  models that  we  have  used  to understand  the  climate  system  have never  been  run  as  in  this  way  where, where  you  look  at  all  of the  different  possible  omes. But  we  do  have quite  good  what  are called  simple  climate  models. Simple  climate  models  or  models  that don't  represent  every  grid  cell  of  the  Earth, but  represent  all  the  main  features of  the  global  climate  system. We  can  read  those  many  times. This  is  coming  from  one  of those  clem  scope  there. And  these  are  the  distributions that  come  out  of  it. For  high mitigation  scenario  low  temperature  change and a  low  mitigation scenario  high  temperature  change. You  can  see  that  the T's  aren't  as  bad  as  what  I  just  showed  you, probity  of  still  getting more  than  three  degrees  of  warming despite  having enormous  global  economy  changing, international  response  to  climate  change. They  will  still  have  more  than  three  degrees. Warming  is  about  percent. 10%  is  a  small  number,  but  it's In  many  contexts,  the  chance  you have  2%  chance  of  something, lots  of  people  are  going  to  die if  in  three  degree  world. And  the  chance  of  lots  of  people die  mean  two  half  percent. Despite  all  the  effort  that  we  a  little  bit scary  if  we're  in  a  high  emission  stereo, the  chance  that  we  have  warming  of  90  degrees is  also  about  2.5%  99  degree  warming  world. Well  it's  on  the  1,500  degrees  warming world  would  still  be  cat. So  the  feedback  to  the  botchange include  things  like  water  vapor  feedback, Arctic  sea  ice  mount  rates, methyl  release,  biosker  tipping  points. We  don't  know  any  of  those  very  well. So  the  strength  of  the  feedback  loops that  we're  talking  about  is  not  well  known. And  uncertainty  in  the  feedback  process results  in  long  til  distributions. The  chance  of  very  high  role  is not  follows,  everything  said. That  s  all  right, I  got  15  minutes, which  means  I  can  talk about  all  the  other  ser, on  any  questions,  on, on  this  chunk  of  the,  say, the  scary  thing  is  like the  interaction  between different  tipping  points,  right? So  like  we  can  determine, maybe  with  lots  of  uncertainty  still, if  this  tipping  point  occurs, then  it  will  increase  temperature  certain. But  how  does  that  tipping  point  interact with  other  tipping  points? That's,  that's  where  there's even  more  uncertainty. But  it's  a, there's  the  potential  for sort  of  having  a  cascade of  these  tipping  points, is  there?  There's  a  potential  there. We  don't  know  how the  Earth  responds  to some  of  the  changes  that  we're  imposing  on. Other  than  to  say  that  the  Earth look  very  different  curtains  its  history. How  did  climate  scientists  like? I  don't  know  if  they  do,  but  do they  characterize  the  surgery? How  do  you  know  how  they  do  that? The  ICC  has  this  two  axis  way of  they  talk  about likelihood  of  something  and  confidence in  how  sure  you are  that  it's  within  that  likelihood. The  reason  we  need  both  of  those  axes  is exactly  because  climate  scientists  are  like, we  don't  know  if we're  primatizing  this  correctly. If  conditional  is  primatized  correctly, we  think  that,  for  example, that  the  CS  is  between  2.5  and  four  aries. The  likelihood  that  we've primatized  correctly  is  median. That's,  you  see  those  kind  of  things  as  so, it's  mostly  their  expert  opinion. The  confidence  level  is absolutely  their  expert  opinion. The  likelihood  levels  are  coming  from sophisticated  models  and  good  analysis. There  are  in  some  cases, we  have  multiple  different  models  that  map out  some  of  the  range of  discrimment  amongst  experts. We  can  turn  those  conference  holes  into a  more  quantitative  thing. For  example,  you  often  see  things  like do  two  thirds  of  the  GCMs, let  me  have  agree  on whether  this  particular  part of  the  Earth  is  going  to  get  warmer, is  going  to  get  wetter  or drier  On  two  thirds  of  the  GCMs  agrees. A  third  of  the  GCMs  are claiming  a  completely  different  story, but  that's  considered  pretty good  as  far  as  confidence  is  concerned. What's  the  CMol? That  one  You  will  play around  with  some  of  the  results  coming out  of  GCMs  and  look  at  some of  the  descriptions,  okay? All  right,  so uncertainty  is  the  fact  of  climate. Just  to  show  this  in  another  way, these  are  some  of  these  scenarios. Ssp  126  being  the  scenario  that, that  basically  keeps  us a  rail  two  degree C  by  the  end  of  the  century. So  it  doesn't  make  us  break the  boat  on  the  Paris  Agreement. The  most  common  as  of  last  few  years. Business  as  usual,  trajectory  is  SP  37370. Sp  370  is  this  orange  one. And  it  gets  up  to  about  five  degrees, four  to  five  degrees  of  form  by  the  century, SSP  25  is  the  middle  of  the  road. That  is,  it's  a little  bit  weird  to  talk  about  the  middle  of the  road  in  this  situation because  SP  245  is  a  world where  we  are  actually  being more  ambitious  future  then  we  are  right  now. But  we  don't  know  where that  emission  is  coming  from. Or  just  presume  that  that  we probably  have  the  potential to  more  ambitious  as  we're going  to  set  the  middle  of  the  road to  meet  the  situation. Currently,  our  historical  scenarios have  been  more  or  less following  the  business  as  usual  structure. Most  of  what  we've  done in  reducing  two  emissions,  for  example, by  revolutions  that  have  happened  in solar  and  wind  power  have not  budged  us  that  far from  the  worst  case  scenario. Next  point  is  I  said this  high  mitigation  scenarios, 2126  keeps  us  within  Paris. But  what  actually  mean  by  that is  the  average  temperature  of, the  expected  temperature  at the  end  of  the  century  is about  two  degrees  of  warming, which  means  that  there's a  50%  chance  that  will  be  above two  degrees  percent  chance will  below  two  degrees  of  forming. So  if  we  actually  wanted to  have  a  decent  probability of  staying  below  the international  communities  target of  stay  below  two  degrees  warming. Instead  what  people  talk  about is  it's  a  different  scenario. It's  SSP  119. But  what  they  talk  about  is a  scenario  where  the  probability  that  we stay  under  two  degrees  warming is  two  thirds  rather than  even  if  we do  something  even  more  extreme than  the  necessity  16. We  cannot  guarantee  that we'll  stay  under  two  degrees  warming. We  can  just  increase,  like I've  been  talking  about uncertainty  in  a  completely  general  sense. But  there  are  many  different forms  of  uncertainty  and  they enter  into  the  things that  we  care  about  in  different  ways. I'm  going  to  quick  go through  a  big  list  of  them, give  you  a  sense  of  what  they  look  like. On  the  left  here  is the  RepenertayI'ming  talk  about what  they  need  and then  how  they're  represented  or some  analysis  of  The  first  one is  primary  uncertainty. The  ECS  is  a  result  of  prior  uncertainty. So  you  can  imagine  coming  up  with a  distribution  over  the  value of  the  ECS  and  using  that  in  your  modeling. The  interpretation  of  that is  basically  you  don't  know exactly  what  the  value  that  this  prior. Then  there's  trajectory  uncertainty, which  is  the  butterfly  effect. That  different  runs  of  your  model could  result  in  very  different  outcomes. Often  what  ideal  that  you  do  is, is  that  you  have  mole  runs  of  your  model. That's  the  way  to  know  that  you  have these  sort  of  map  out  structure  uncertainty. And  the  interpretation  of the  fact  that  there's, there's  this  range  of  P comes  is  that  there's  chose. That's,  that's  what  it means  when  you  see  that. Then  there's  error  which  is basically  the  individual  runs of  your  model  don't correspond  reality and  that  they  don't  necessarily, the  individual  years  of  your  model might  not  be  reflective  of  actual  future  use. Where  this  is  ignored  a  couple  of  things, but  the  first  is, when  we're  using  these big  global  climate  models, what  we  get  out  of  them  is  something  like the  climatological  average  at the  end  of  the  century. The  average  around  2,100  But  actually those  models  will  give  you out  a  particular  global  warming  in 2,100  And  it'll  be slightly  different  value  from  2089. And  they'll  fluctuate  back  and  forth  all over  the  place  because  that's  what  tempts  do. Actually  we're  not  projecting the  individual  temperature  of  the  year, 2,100  because  we  don't believe  the  models  is  capturing  that. It's  worthwhile,  it  makes  sense  to, but  it  also  means that  there's  a  lot  of  variability. There's  a  difference  between weather  and  climate. That  tells  you  something  about how  much  individual  years  might  suck. Because  you  think  that you're  able  forming  two  degrees, but  then  an  individual  year  gives  you a  40  reason  or  that  you  have  this  noise. Okay,  So  that's  traditional. What  variable  stuff  that comes  into  the  stuff. Then  you  have  other  notes that  aren't  as  quantifiable. These  all  reflect  some  kind of  depot  certainty  model. Uncertainty  is  when  you have  multiple  different  models  deal  you  have multiple  models  and  you just  to  guess  what  more  Ser  is  utile, different  models  that give  you  different  answers. This  is  patterns  of  results from  an  amino  event  in  a  historical  period. We  don't  even  know  what  a historic  weather  events  came  out  with. Different  models  produce  different  estimates of  what  happened. Model  inadequacy. There  are  limits  that  we  know  about, about  what  our  models  are able  to  reproduce,  what  they  are. This  stuff  that  I've  worked  on where  the  models  in the  middle  that  are  out  where the  US  is  outlined  in, are  different  outcomes  of different  climate  models  showing the  warming  across  the  US. All  these  ones  are basically  fake  models  that  have  to  be added  in  in  order  to  capture  the  CS. Ecs  has  a  Sal, but  actually  the  models  we  have only  produce a  small  portion  in  the  middle  of  that. We  can't,  we  can't, but  people  mostly  haven't  made high  quality  models  that produce  results  that  we know  in  areas  of the  distribution  where  we know  there's  probability  mass. Part  of  this  is  because no  modeling  team  that  runs  well  these  models, we  would  publish  a  set of  results  that  give you  a  warning  of  like  H  trees, they're  just  going  to  throw  that  way because  they  know  that  that's the  most  likely  thing  that's  going  to  happen. Instead  they  just  been  do the  red  and  give  you  the  result. Model  is  limitations  of  the  model. The,  the  Smith  then  broader  depocern, which  is  unquantified  aspects  of  the  problem. Things  that  haven't  been captured  in  any  indel, haven't  even  been  identified necessarily  as  limitations  of  the  model. One  of  the  ways  this  is  represented  in  CC is  what  are  called  reasons  for  concern. Reason  for  concern  is  basically a  change  in  either  the  Earth  system or  social  systems  that  rely  on  it. Unique  systems  like  ecosystems  sums, distribution  of  impacts,  who  gets hurt  more  and  less  large  scale, lars,  the  possibility  of collapses  of  the  earth  system, of  different  aspects  of  the  earth  system. These  are  things  that we  don't  know  much  about, but  there's  expert  opinion on  how  likely  they  are  at  foot  levels. And  so  basically  we're  left  in  a  world where  all  we  can do  is  ask  people  who  study  this  up, do  you  think  this  is  more  or  less  likely? And  at  what  level  of  warning  do  you think  it  becomes  fairly  likely? By  the  way,  almost  all of  these  are  already  at  the  moderate  level of  to talk  about  this  Tuesday. Objections,  this  is  the  last  point to  make  is  just  these  are  different  kinds  of industry  that  affect just  any  given  one  process  that we're  looking  at  when we're  doing  actual  flimsy. We  have  multiple  interacting  levels of  a  represent  zero  systems represent  the  social  systems that  represents  the  economic  system. And  unfortunately,  that  means that  uncertainty's  even  worse.
===