**user**:
> I have a directory with a collection of txt files (there are also html files there I want to ignore), which look like this:
> ```
> 626:            Bottom Trawl &lt; 65 ft
> 627:            Bottom Trawl &gt; 65 ft
> 628:            Dredge
> 629:            Gillnet
> 631:            Longline
> 632:            Pots and Traps
> ```
> 
> I want to go through the lines of all of these txt files and download the content of the URL "https://portal.midatlanticocean.org/data_manager/get_layer_catalog_content/###/" where the ### is the number from the file, and put those in a separate directory. Finally, as I go, I want to build up a CSV file of all of these entries, with columns:
> ID, Name, Category
> Where ID is the ###, Name is the stuff after the colon, and Category is the name of the txt file.
> 
> Can you write this for me?
**assistant**:
> Certainly! Below is a Python script that should accomplish what you want. It iterates through `.txt` files in a specific directory, extracts the numbers and names, downloads content from the specified URL for each number, saves each downloaded content in a separate directory, and creates a CSV file summarizing the information.
> 
> Here's the script:
> 
> ```python
> import os
> import re
> import csv
> import requests
> 
> # Directory containing the txt files
> input_dir = 'path/to/your/txt_files'
> # Directory to save the downloaded content
> output_dir = 'path/to/save/downloaded_content'
> # CSV file to store the data
> csv_file = 'path/to/save/summary.csv'
> 
> # Ensure the output directory exists
> os.makedirs(output_dir, exist_ok=True)
> 
> # Function to process a single file
> def process_file(file_path, category, csv_writer):
>     with open(file_path, 'r') as file:
>         for line in file:
>             match = re.match(r'(\d+):\s+(.+)', line)
>             if match:
>                 id_num = match.group(1)
>                 name = match.group(2).strip()
>                 url = f"https://portal.midatlanticocean.org/data_manager/get_layer_catalog_content/{id_num}/"
>                 
>                 # Download content from the URL
>                 response = requests.get(url)
>                 if response.status_code == 200:
>                     file_name = f"{id_num}.txt"
>                     file_path = os.path.join(output_dir, file_name)
>                     
>                     # Save the content to a file
>                     with open(file_path, 'w') as output_file:
>                         output_file.write(response.text)
>                     
>                     # Write to the CSV file
>                     csv_writer.writerow([id_num, name, category])
>                 else:
>                     print(f"Failed to download content for ID {id_num}")
> 
> # Create and open the CSV file for writing
> with open(csv_file, 'w', newline='') as csvfile:
>     csv_writer = csv.writer(csvfile)
>     csv_writer.writerow(['ID', 'Name', 'Category'])
>     
>     # Iterate through all files in the input directory
>     for filename in os.listdir(input_dir):
>         if filename.endswith('.txt'):
>             file_path = os.path.join(input_dir, filename)
>             category = os.path.splitext(filename)[0]  # Use the filename without the extension as the category
>             process_file(file_path, category, csv_writer)
> 
> print("Processing complete.")
> ```
> 
> ### How to Use the Script
> 1. **Update Directory Paths**:
>    - Replace `path/to/your/txt_files` with the actual path where your `.txt` files are located.
>    - Replace `path/to/save/downloaded_content` with the path where you want to save the downloaded content.
>    - Replace `path/to/save/summary.csv` with the path and filename for the output CSV file.
> 
> 2. **Run the Script**:
>    - Make sure you have Python installed and the required modules (`requests`) available.
>    - Save the script to a file, e.g., `process_files.py`.
>    - Run the script using `python process_files.py`.
> 
> The script will iterate through all `.txt` files in the specified directory, download the URLs, save the content, and build the CSV file as specified. Let me know if you have any questions or need further tweaks!
