In a conversation about building a random forest algorithm with distributional observations, James raised a concern about the Anderson-Darling 2-sample test leading to small splits in the model. Arachne suggested several approaches to incentivize larger splits in the random forest:

1. Adjusting the threshold of the test to differentiate between significantly different distributions, potentially resulting in larger partitioned sets.
2. Adding a regularization term that penalizes smaller splits by multiplying the test statistic by the size of the smaller set.
3. Using divergence measures like Kullback-Leibler Divergence or Jensen-Shannon Divergence as an alternative statistic for calculating the "goodness" of a split, as they are less sensitive to extreme values.
4. Performing the Anderson-Darling test on bootstrap resamples instead of raw samples to smooth out the influence of individual observations.
5. Combining the Anderson-Darling test with another splitting measure, such as a weighted sum or using the Anderson-Darling test only under certain conditions.

Arachne warned that there is a trade-off between split integrity and the size of resulting subsets, requiring careful tuning and consideration of the model's performance.
